{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:40:28.593758Z",
     "start_time": "2025-04-29T02:40:22.870102Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "og7M_A3y9PhT",
    "outputId": "a75bdd5a-b677-4cde-d0af-98a4691d6260"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/29 05:14:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (col, lit, from_unixtime, to_timestamp,\n",
    "                                   year, weekofyear, hour, concat_ws)\n",
    "from pyspark.sql import Window, functions as F\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "     when, size, collect_set\n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .master(\"spark://spark:7077\")\n",
    "      .appName(\"WiClean+\")\n",
    "      .config(\"spark.driver.memory\",   \"16g\")\n",
    "      .config(\"spark.executor.memory\", \"16g\")\n",
    "      .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "      .config(\"spark.default.parallelism\",     \"200\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# sanity check to make sure spark works in docker\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:41:04.582238Z",
     "start_time": "2025-04-29T02:41:03.388772Z"
    },
    "id": "GLmOiOBL9dwQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "links_df    = spark.read.parquet(\"data/links.parquet\")\n",
    "articles_df = spark.read.parquet(\"data/articles.parquet\")\n",
    "\n",
    "assert links_df.count() > 0, \"links table empty!\"\n",
    "assert articles_df.count() > 0, \"articles table empty!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:41:34.062994Z",
     "start_time": "2025-04-29T02:41:11.117809Z"
    },
    "id": "53VUQKfg9kxj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>  (381 + 19) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rows : 310810529\n",
      "+-----------+-----------+-------------------+------+\n",
      "|src_article|dst_article|timestamp          |action|\n",
      "+-----------+-----------+-------------------+------+\n",
      "|477        |7711       |2023-01-12 16:37:35|+     |\n",
      "|477        |1935       |2023-01-12 16:37:35|+     |\n",
      "|477        |11632      |2023-01-12 16:37:35|+     |\n",
      "|477        |8309       |2023-01-12 16:37:35|+     |\n",
      "|477        |11633      |2023-01-12 16:37:35|+     |\n",
      "+-----------+-----------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "adds = (links_df\n",
    "            .select(\"src_article\",\"dst_article\",\n",
    "                    to_timestamp(from_unixtime(col(\"created_at\"))).alias(\"timestamp\"))\n",
    "            .withColumn(\"action\", lit(\"+\")))\n",
    "removes = (links_df\n",
    "               .select(\"src_article\",\"dst_article\",\n",
    "                       to_timestamp(from_unixtime(col(\"removed_at\"))).alias(\"timestamp\"))\n",
    "               .withColumn(\"action\", lit(\"-\")))\n",
    "events = (adds.union(removes)\n",
    "              .filter((col(\"timestamp\") >= \"2023-01-01\") &\n",
    "                      (col(\"timestamp\") <  \"2025-01-01\")))\n",
    "\n",
    "print(\"Event rows :\", events.count())\n",
    "events.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:42:42.198097Z",
     "start_time": "2025-04-29T02:42:04.290974Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiB6T7VcUbQE",
    "outputId": "c689e567-5a0f-4a81-f831-8a98573209a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===================================================> (390 + 10) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " events persisted to data/events.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events.write \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .parquet(\"data/events.parquet\")\n",
    "print(\" events persisted to data/events.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:44:29.418451Z",
     "start_time": "2025-04-29T02:43:28.248964Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Et7Uu4IbAL8J",
    "outputId": "f0e51725-f38d-4224-f3a6-7838df668bd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(397 + 3) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-500,000 pages generate 45.94% of all link-edit events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TOP_K = 500_000\n",
    "\n",
    "# build a DataFrame of the TOP_K most-edited source articles\n",
    "topk_src = (events\n",
    "            .groupBy(\"src_article\")\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc())\n",
    "            .limit(TOP_K)\n",
    "            .cache())\n",
    "\n",
    "# compute what fraction of all add/remove events\n",
    "covered = events.join(topk_src, on=\"src_article\").count()\n",
    "total   = events.count()\n",
    "print(f\"Top-{TOP_K:,} pages generate {covered/total:6.2%} of all link-edit events\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:45:53.252577Z",
     "start_time": "2025-04-29T02:44:43.185537Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y--tSBIvDxVc",
    "outputId": "ddbd0947-a2cf-439a-e6e5-0646e9605268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:45:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/28 20:45:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/28 20:45:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/28 20:45:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/28 20:45:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/28 20:45:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages needed for 90 % coverage â‰ˆ 1,495,665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:==============================================>       (86 + 14) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage with K=1,495,665: 69.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rank all src pages by event count\n",
    "counts = (events.groupBy(\"src_article\")\n",
    "                  .count()\n",
    "                  .orderBy(col(\"count\").desc())\n",
    "                  .cache())\n",
    "\n",
    "total_events = events.count()\n",
    "\n",
    "# running cumulative sum\n",
    "w = Window.orderBy(col(\"count\").desc())\n",
    "counts = counts.withColumn(\"cum_events\",\n",
    "                           F.sum(\"count\").over(w)) \\\n",
    "               .withColumn(\"cum_pct\",\n",
    "                           col(\"cum_events\") / total_events)\n",
    "\n",
    "# how many pages to hit 90 % of traffic?\n",
    "target_rows = counts.filter(col(\"cum_pct\") <= 0.70).count()\n",
    "print(f\"Pages needed for 90 % coverage â‰ˆ {target_rows:,}\")\n",
    "\n",
    "# generate DataFrame to pass to the Wikidata fetch\n",
    "topk_src = counts.limit(target_rows).select(\"src_article\").cache()\n",
    "\n",
    "# sanity-print actual share\n",
    "covered = events.join(topk_src, on=\"src_article\").count()\n",
    "print(f\"Coverage with K={target_rows:,}: {covered/total_events:6.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:49:43.032624Z",
     "start_time": "2025-04-29T02:46:58.508264Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NreMGceE9pW7",
    "outputId": "30daa37f-9ed8-42c9-c7df-c9ad697a996d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:46:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in new weekly_feats: 23979591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+-----------+-------+--------+------------+\n",
      "|src_article|week   |total_edits|adds|removes|unique_targets|revert_count|odd_hour_edits|is_disambig|is_list|is_index|is_year_page|\n",
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+-----------+-------+--------+------------+\n",
      "|1177751    |2023-18|52         |23  |29     |53            |0           |0             |0          |0      |0       |0           |\n",
      "|3443144    |2024-42|1          |1   |0      |1             |0           |0             |0          |0      |0       |0           |\n",
      "|3474916    |2024-32|1          |1   |0      |1             |0           |1             |0          |0      |0       |0           |\n",
      "|3474916    |2024-32|1          |1   |0      |1             |0           |1             |0          |0      |0       |0           |\n",
      "|3474916    |2023-4 |24         |17  |7      |24            |1           |0             |0          |0      |0       |0           |\n",
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+-----------+-------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#load\n",
    "events = spark.read.parquet(\"data/events.parquet\").cache()\n",
    "articles = spark.read.parquet(\"data/articles.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add week & hour\n",
    "with_dt = (\n",
    "    events\n",
    "      .withColumn(\"week\", concat_ws(\"-\", year(\"timestamp\"), weekofyear(\"timestamp\")))\n",
    "      .withColumn(\"hour\", hour(\"timestamp\"))\n",
    ")\n",
    "\n",
    "#  base features per (src,week)\n",
    "base = (\n",
    "    with_dt.groupBy(\"src_article\",\"week\")\n",
    "           .agg(\n",
    "             F.count(\"*\").alias(\"total_edits\"),\n",
    "             F.sum(when(col(\"action\")==\"+\",1).otherwise(0)).alias(\"adds\"),\n",
    "             F.sum(when(col(\"action\")==\"-\",1).otherwise(0)).alias(\"removes\"),\n",
    "             F.approx_count_distinct(\"dst_article\").alias(\"unique_targets\")\n",
    "           )\n",
    ")\n",
    "\n",
    "# compute revert_count per (src,week)\n",
    "act_sets = (\n",
    "    with_dt.groupBy(\"src_article\",\"week\",\"dst_article\")\n",
    "           .agg(collect_set(\"action\").alias(\"acts\"))\n",
    ")\n",
    "reverts = (\n",
    "    act_sets.filter(size(\"acts\")==2)\n",
    "            .groupBy(\"src_article\",\"week\")\n",
    "            .count()\n",
    "            .withColumnRenamed(\"count\",\"revert_count\")\n",
    ")\n",
    "\n",
    "# compute odd_hour_edits per (src,week)\n",
    "odd = (\n",
    "    with_dt.filter((col(\"hour\")<6)|(col(\"hour\")>22))\n",
    "           .groupBy(\"src_article\",\"week\")\n",
    "           .count()\n",
    "           .withColumnRenamed(\"count\",\"odd_hour_edits\")\n",
    ")\n",
    "\n",
    "# join them all together, filling missing with 0\n",
    "weekly_feats = (\n",
    "    base.join(reverts, [\"src_article\",\"week\"], \"left\")\n",
    "        .join(odd,     [\"src_article\",\"week\"], \"left\")\n",
    "        .na.fill({\"revert_count\":0, \"odd_hour_edits\":0})\n",
    ")\n",
    "\n",
    "# derive page-type flags from title\n",
    "pages = (\n",
    "    articles\n",
    "      .withColumnRenamed(\"id\",\"src_article\")\n",
    "      .withColumn(\"is_disambig\",\n",
    "         col(\"title\").rlike(r\"\\(disambiguation\\)$\").cast(\"int\"))\n",
    "      .withColumn(\"is_list\",\n",
    "         col(\"title\").rlike(r\"(?i)^List of \").cast(\"int\"))\n",
    "      .withColumn(\"is_index\",\n",
    "         col(\"title\").rlike(r\"(?i)^Index of \").cast(\"int\"))\n",
    "      .withColumn(\"is_year_page\",\n",
    "         col(\"title\").rlike(r\"^\\d{4}(â€“\\d{4})?$\").cast(\"int\"))\n",
    "      .select(\"src_article\",\"is_disambig\",\"is_list\",\"is_index\",\"is_year_page\")\n",
    ")\n",
    "\n",
    "# attach page-type to the features\n",
    "weekly_feats = weekly_feats.join(pages, on=\"src_article\", how=\"left\") \\\n",
    "                           .na.fill(0, subset=[\"is_disambig\",\"is_list\",\"is_index\",\"is_year_page\"])\n",
    "\n",
    "\n",
    "weekly_feats.write.mode(\"overwrite\").parquet(\"data/weekly_feats_v2.parquet\")\n",
    "\n",
    "print(\"Rows in new weekly_feats:\", weekly_feats.count())\n",
    "weekly_feats.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:52:17.177553Z",
     "start_time": "2025-04-29T02:51:47.075218Z"
    },
    "id": "-kFrcLHX_6EZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article types extracted (deduped): 10,704,167 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:=========================================>          (159 + 32) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|article_id|type         |\n",
      "+----------+-------------+\n",
      "|1313149   |battle       |\n",
      "|1750043   |battle       |\n",
      "|1324355   |manager      |\n",
      "|9342802   |location     |\n",
      "|11011132  |successor    |\n",
      "|13019472  |category     |\n",
      "|5442931   |homeStadium  |\n",
      "|12550808  |city         |\n",
      "|1507531   |artist       |\n",
      "|434776    |country      |\n",
      "|502222    |starring     |\n",
      "|640764    |location     |\n",
      "|5626903   |producer     |\n",
      "|2077122   |hubAirport   |\n",
      "|2094478   |parentCompany|\n",
      "|4182488   |deathPlace   |\n",
      "|239681    |recordLabel  |\n",
      "|11189235  |birthPlace   |\n",
      "|12774053  |occupation   |\n",
      "|4038285   |birthPlace   |\n",
      "+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, lower, regexp_replace\n",
    "\n",
    "# load and parse instance-types file\n",
    "lines = spark.read.option(\"compression\", \"bzip2\") \\\n",
    "                  .text(\"data/instance-types_lang=en_specific.ttl.bz2\")\n",
    "\n",
    "triples = lines.filter(\n",
    "    col(\"value\").rlike(r\"^<http://dbpedia\\.org/resource/[^>]+>\\s+\" +\n",
    "                       r\"<http://dbpedia\\.org/ontology/[^>]+>\\s+\" +\n",
    "                       r\"<http://dbpedia\\.org/resource/[^>]+>\\s+\\.$\")\n",
    ")\n",
    "\n",
    "inst = triples.select(\n",
    "    regexp_extract(\"value\", r\"^<http://dbpedia\\.org/resource/([^>]+)>\", 1).alias(\"resource\"),\n",
    "    regexp_extract(\"value\", r\"<http://dbpedia\\.org/ontology/([^>]+)>\", 1).alias(\"type\")\n",
    ").withColumn(\"resource_lc\", lower(\"resource\"))\n",
    "\n",
    "# reload articles\n",
    "articles = spark.read.parquet(\"data/articles.parquet\") \\\n",
    "                     .select(col(\"id\").alias(\"article_id\"), \"title\") \\\n",
    "                     .withColumn(\"resource\", regexp_replace(\"title\", \" \", \"_\")) \\\n",
    "                     .withColumn(\"resource_lc\", lower(\"resource\"))\n",
    "\n",
    "# match types to articles\n",
    "article_types_specific = (inst.join(articles, on=\"resource_lc\", how=\"inner\")\n",
    "                              .select(\"article_id\", \"type\"))\n",
    "\n",
    "#  remove duplicate (article_id, type) pairs\n",
    "article_types_specific = article_types_specific.dropDuplicates([\"article_id\", \"type\"])\n",
    "\n",
    "# Save\n",
    "article_types_specific.write.mode(\"overwrite\") \\\n",
    "                           .parquet(\"data/article_types_specific.parquet\")\n",
    "\n",
    "print(f\"Article types extracted (deduped): {article_types_specific.count():,} rows\")\n",
    "article_types_specific.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:52:46.745680Z",
     "start_time": "2025-04-29T02:52:37.751867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS0mGC3Nq_Or",
    "outputId": "b84ffdc1-9203-4468-ef9c-b5dad63581a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:===============================>                    (122 + 32) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|article_id|type           |\n",
      "+----------+---------------+\n",
      "|1313149   |battle         |\n",
      "|1750043   |battle         |\n",
      "|1324355   |manager        |\n",
      "|9342802   |location       |\n",
      "|11011132  |successor      |\n",
      "|13019472  |category       |\n",
      "|5442931   |homeStadium    |\n",
      "|12550808  |city           |\n",
      "|1507531   |artist         |\n",
      "|434776    |country        |\n",
      "|502222    |starring       |\n",
      "|640764    |location       |\n",
      "|5626903   |producer       |\n",
      "|2077122   |hubAirport     |\n",
      "|2094478   |parentCompany  |\n",
      "|4182488   |deathPlace     |\n",
      "|239681    |recordLabel    |\n",
      "|11189235  |birthPlace     |\n",
      "|12774053  |occupation     |\n",
      "|4038285   |birthPlace     |\n",
      "|9287577   |almaMater      |\n",
      "|15008672  |class          |\n",
      "|15022301  |diocese        |\n",
      "|6827731   |party          |\n",
      "|16775301  |division       |\n",
      "|2034549   |timeZone       |\n",
      "|3246079   |locationCountry|\n",
      "|447483    |composer       |\n",
      "|12087338  |regionServed   |\n",
      "|16516942  |keyPerson      |\n",
      "+----------+---------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "article_types_specific.show(30,truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:53:09.387730Z",
     "start_time": "2025-04-29T02:53:08.493316Z"
    },
    "id": "lUDAm26yA8SD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Meaningful article types extracted: 1,756,334 rows\n",
      "+----------+--------+\n",
      "|article_id|type    |\n",
      "+----------+--------+\n",
      "|8008769   |country |\n",
      "|473035    |country |\n",
      "|4812727   |country |\n",
      "|5825702   |team    |\n",
      "|6936722   |album   |\n",
      "|10759919  |team    |\n",
      "|458025    |artist  |\n",
      "|18632216  |location|\n",
      "|4804315   |location|\n",
      "|16844646  |location|\n",
      "+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the cleaned types file\n",
    "article_types_specific = spark.read.parquet(\"data/article_types_specific.parquet\")\n",
    "\n",
    "#  special meaningful types based on WiClean paper (manually curated list)\n",
    "meaningful_types = [\n",
    "    \"person\", \"soccerPlayer\", \"actor\", \"musician\", \"artist\",\n",
    "    \"country\", \"city\", \"organization\", \"company\", \"film\",\n",
    "    \"movie\", \"location\", \"team\", \"writer\", \"album\",\n",
    "    \"language\", \"university\", \"televisionShow\", \"politician\"\n",
    "]\n",
    "\n",
    "# lowercase everything (matches normalization applied in scraper)\n",
    "from pyspark.sql.functions import lower\n",
    "article_types_specific = article_types_specific.withColumn(\"type\", lower(col(\"type\")))\n",
    "\n",
    "# filter types\n",
    "article_types_filtered = article_types_specific.filter(col(\"type\").isin(meaningful_types))\n",
    "\n",
    "# save filtered types\n",
    "article_types_filtered.write.mode(\"overwrite\") \\\n",
    "                            .parquet(\"data/article_types_filtered.parquet\")\n",
    "\n",
    "print(f\" Meaningful article types extracted: {article_types_filtered.count():,} rows\")\n",
    "article_types_filtered.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:56:02.346587Z",
     "start_time": "2025-04-29T02:53:35.133925Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "DrLWPR3b5LK8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:53:35 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Weekly features generated: 21,498,555 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+\n",
      "|src_article|week   |total_edits|adds|removes|unique_targets|revert_count|odd_hour_edits|\n",
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+\n",
      "|206823     |2024-42|1          |1   |0      |1             |0           |0             |\n",
      "|1235884    |2024-32|8          |4   |4      |4             |4           |4             |\n",
      "|1281794    |2024-2 |104        |52  |52     |106           |0           |0             |\n",
      "|2011965    |2023-3 |1          |1   |0      |1             |0           |0             |\n",
      "|7043377    |2024-9 |3          |2   |1      |2             |1           |0             |\n",
      "+-----------+-------+-----------+----+-------+--------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# reload events\n",
    "events = spark.read.parquet(\"data/events.parquet\").cache()\n",
    "\n",
    "# add week and hour\n",
    "with_dt = (events.withColumn(\"week\", concat_ws(\"-\", year(\"timestamp\"), weekofyear(\"timestamp\")))\n",
    "                   .withColumn(\"hour\", hour(\"timestamp\")))\n",
    "\n",
    "# base features\n",
    "base = (with_dt.groupBy(\"src_article\", \"week\")\n",
    "             .agg(\n",
    "                 F.count(\"*\").alias(\"total_edits\"),\n",
    "                 F.sum(when(col(\"action\") == \"+\", 1).otherwise(0)).alias(\"adds\"),\n",
    "                 F.sum(when(col(\"action\") == \"-\", 1).otherwise(0)).alias(\"removes\"),\n",
    "                 F.approx_count_distinct(\"dst_article\").alias(\"unique_targets\")\n",
    "             ))\n",
    "\n",
    "# reverts\n",
    "act_sets = (with_dt.groupBy(\"src_article\", \"week\", \"dst_article\")\n",
    "                  .agg(collect_set(\"action\").alias(\"acts\")))\n",
    "\n",
    "reverts = (act_sets.filter(size(\"acts\") == 2)\n",
    "                    .groupBy(\"src_article\", \"week\")\n",
    "                    .count()\n",
    "                    .withColumnRenamed(\"count\", \"revert_count\"))\n",
    "\n",
    "# odd hour edits\n",
    "odd = (with_dt.filter((col(\"hour\") < 6) | (col(\"hour\") > 22))\n",
    "             .groupBy(\"src_article\", \"week\")\n",
    "             .count()\n",
    "             .withColumnRenamed(\"count\", \"odd_hour_edits\"))\n",
    "\n",
    "# final join\n",
    "weekly_feats = (base.join(reverts, [\"src_article\", \"week\"], \"left\")\n",
    "                    .join(odd, [\"src_article\", \"week\"], \"left\")\n",
    "                    .na.fill({\"revert_count\": 0, \"odd_hour_edits\": 0}))\n",
    "\n",
    "# save\n",
    "weekly_feats.write.mode(\"overwrite\").parquet(\"data/weekly_feats_v23.parquet\")\n",
    "\n",
    "print(f\" Weekly features generated: {weekly_feats.count():,} rows\")\n",
    "weekly_feats.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T02:56:42.406043Z",
     "start_time": "2025-04-29T02:56:37.331418Z"
    },
    "id": "MJyAc5w2ey5E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10,000 meaningful typed pages saved: 10,000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:================================>                    (61 + 32) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+\n",
      "|article_id|sum_edits|type    |\n",
      "+----------+---------+--------+\n",
      "|179793    |36038    |location|\n",
      "|179793    |36038    |company |\n",
      "|40339     |30668    |language|\n",
      "|62        |28185    |language|\n",
      "|1678533   |27671    |country |\n",
      "|9855      |20037    |country |\n",
      "|1332454   |19620    |country |\n",
      "|383352    |18736    |country |\n",
      "|383352    |18736    |company |\n",
      "|145866    |17602    |writer  |\n",
      "+----------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# load the clean weekly features\n",
    "weekly_feats = spark.read.parquet(\"data/weekly_feats_v23.parquet\")\n",
    "\n",
    "# load the meaningful filtered article types\n",
    "article_types = spark.read.parquet(\"data/article_types_filtered.parquet\")\n",
    "\n",
    "# compute total edits per page\n",
    "total_edits = (weekly_feats.groupBy(\"src_article\")\n",
    "                           .agg(spark_sum(\"total_edits\").alias(\"sum_edits\")))\n",
    "\n",
    "# join with article types\n",
    "typed_pages = (total_edits.join(article_types,\n",
    "                                total_edits.src_article == article_types.article_id,\n",
    "                                how=\"inner\")\n",
    "                         .select(\"article_id\", \"sum_edits\", \"type\"))\n",
    "\n",
    "# select top 10,000 pages by edit count\n",
    "top10k = typed_pages.orderBy(col(\"sum_edits\").desc()).limit(10000)\n",
    "\n",
    "# save for later use\n",
    "top10k.write.mode(\"overwrite\").parquet(\"data/top10k_typed_pages.parquet\")\n",
    "\n",
    "print(f\"Top-10,000 meaningful typed pages saved: {top10k.count():,} rows\")\n",
    "top10k.show(10, truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
